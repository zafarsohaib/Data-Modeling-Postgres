
# Data Sets

There are two datasets used as input to the ETL (extract, transform and load) processing of data into the postgreSQL database.

- Song Dataset

	The first dataset is a subset of real data from the  [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. 

- Log Dataset
	The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

# Database Schema Overview

For our startup app "Sparkify", we create a database schema "sparkifydb" in PostgreSQL database.  The database can later be used to write queries needed for song play analysis. The sparkify database is designed to be a start schema with one fact table and four dimension tables as follows:
    
- Fact Table:

	**songplays table:**
	records in log data associated with song plays i.e. records with page *NextSong*.
	*columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent.*

- Dimension Tables:

	**users table:**
records of users in the app.
*columns: user_id, first_name, last_name, gender, level.*
   
	**songs table:**
records of songs in music database.
*columns: song_id, title, artist_id, year, duration.*
   
	**artists table**
 records artists in music database.
*columns: artist_id, name, location, latitude, longitude.*

	**time table:**
timestamps of records in songplays broken down into specific units.
*columns: start_time, hour, day, week, month, year, weekday.*

    
# Python Scripts

There are 3 python scripts writen to achieve the ETL processing of data.

1. sql_queries.py:

	This python file contains all the sql queries needed for dropping, creating, selecting and inserting into tables. These queries are used by the other two python scripts.
    
2. create_tables.py:

	This python script performs the following tasks:
    - Drops (if exists) and creates the *sparkify* database. 
    - Establishes connection with the sparkify database and gets cursor to it.  
    - Drops all the tables.  
    - Creates all empty tables needed. 
    - Finally, closes the connection. 
    
3. <span>etl.py</span>:

	The pyton script performs the following tasks:

    - Connects to *"sparkify"* database.    
    - Iterates through all the files in the *'song directory'* and inserts json files content to *songs* and *artists* tables of database. 
    - Iterates through all the files in the *'log directory'* and inserts json files content to *songplays*, *time* and *users* tables of database.
    - Finally, closes the connection.  


# Steps to Setup Database


1. Create tables by running *"create_table.py".*
2. Process json files in *song* and *log* directories and insert them into database tables by running *"<span>etl.py</span>"*.


# SQL Example Queries


After the data is inserted in database you can use the database to fetch different information. In the section below you can find queries for different use-cases:

But first connect to database as follows:

	>> %load_ext sql
	>> %sql postgresql://student:student@127.0.0.1/sparkifydb

Folowing query will give the list of user names who listened to a particular song *(for example, "Setanta matins")*:

	>> %sql SELECT u.first_name, u.last_name FROM songplays sp inner join users u on u.user_id=sp.user_id inner join songs s on s.song_id=sp.song_id where s.title='Setanta matins'

A query to find the list of paid members of *Sparkify* app, sorted by first name would be:

	>> %sql SELECT first_name, last_name FROM users where level='paid' order by first_name

Following query can be used to find the date when a particular user last accessed the app and clicked on *"NextSong"* *(for example, the user name 'Aleena'):*

	>> %sql SELECT t.day || '-' || t.month  || '-' || t.year as LAST_ACCCESS_DATE FROM songplays sp inner join time t on t.start_time = sp.start_time inner join users u on u.user_id=sp.user_id where u.first_name='Aleena' order by sp.start_time desc limit 1

*Hint:* If a query is used often but its response time is slow there are few things that can be done to speed up query. First, the indexes can be created on the attributes in the *where* clause of the query. Secondly, if there are many joins in the query, we can opt for a denormalized table for fast query execution. In case of a denormalized schema, the issues related to data integrity needs has to be taken care.
